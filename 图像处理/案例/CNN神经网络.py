import torch
import torch.nn as nn
import numpy as np

#使用pytorch的1维卷积层

# Conv1d
# class torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)

# in_channels(int) – 输入信号的通道。在文本分类中，即为词向量的维度
# out_channels(int) – 卷积产生的通道。有多少个out_channels，就需要多少个1维卷积
# kerner_size(int or tuple) - 卷积核的尺寸，卷积核的大小为(k,)，第二个维度是由in_channels来决定的，所以实际上卷积大小为in_channels * kerner_size
# stride(int or tuple, optional) - 卷积步长
# padding (int or tuple, optional)- 输入的每一条边补充0的层数
# dilation(int or tuple, `optional``) – 卷积核元素之间的间距
# groups(int, optional) – 从输入通道到输出通道的阻塞连接数
# bias(bool, optional) - 如果bias=True，添加偏置

# 设置随机数种子
torch.manual_seed(0)

input_dim = 6
hidden_size = 8
kernel_size = 2
# hidden_size * input_dim * kernel_size
torch_cnn1d = nn.Conv1d(input_dim, hidden_size, kernel_size)
for key, weight in torch_cnn1d.state_dict().items():
    print(key, weight.shape) # 随机初始化
# weight torch.Size([8, 6, 2]) # 卷积核是6 * 2的
# bias torch.Size([8])
# 卷积核：每一个6 * 2
#  tensor([[[-0.0022,  0.1549],
#          [-0.2376, -0.2124],
#          [-0.1112,  0.0774],
#          [-0.0057,  0.2289],
#          [-0.0256,  0.0764],
#          [-0.0872, -0.0567]],
#         [[-0.2758, -0.1912],
#          [-0.1190,  0.0107],
#          [ 0.1141,  0.1732],
#          [-0.1957, -0.1257],
#          [ 0.1049,  0.2397],
#          [-0.0594,  0.2160]],
# bias tensor([ 0.1441,  0.0604, -0.2252, -0.1662,  0.2716,  0.1945, -0.1259, -0.0727])

x = torch.rand((6,8)) #embedding_size * max_length
# tensor([[0.0237, 0.4910, 0.1235, 0.1143, 0.4725, 0.5751, 0.2952, 0.7967],
#         [0.1957, 0.9537, 0.8426, 0.0784, 0.3756, 0.5226, 0.5730, 0.6186],
#         [0.6962, 0.5300, 0.2560, 0.7366, 0.0204, 0.2036, 0.3748, 0.2564],
#         [0.3251, 0.0902, 0.3936, 0.6069, 0.1743, 0.4743, 0.8579, 0.4486],
#         [0.5139, 0.4569, 0.6012, 0.8179, 0.9736, 0.8175, 0.9747, 0.4638],
#         [0.0508, 0.2630, 0.8405, 0.4968, 0.2515, 0.1168, 0.0321, 0.0780]])
# print(x)
y = torch_cnn1d(x)
print(y,y.shape, 'y') 
# torch.Size([8, 7])
# 结果
# tensor([[-0.0442, -0.2293,  0.0555,  0.0706,  0.1619,  0.1843,  0.0785],
#         [ 0.2604,  0.1926,  0.2250,  0.2241,  0.0333, -0.0038, -0.1436],
#         [-0.8331, -0.6933, -0.5464, -0.9069, -0.5644, -0.4783, -0.5736],
#         [-0.0874, -0.1037, -0.2986, -0.1972, -0.3900, -0.5917, -0.2225],
#         [ 0.5973,  0.3586,  0.0708,  0.2365,  0.1894,  0.2374,  0.5821],
#         [ 0.5845,  0.3864,  0.2402,  0.6972,  0.6665,  0.7329,  0.6588],
#         [ 0.1000,  0.1580, -0.0327,  0.2036,  0.1310,  0.1089,  0.1399],
#         [-0.1046, -0.1999, -0.2098,  0.1102,  0.0889,  0.2122,  0.1774]],

kernel_a = torch.FloatTensor([[-0.0022,  0.1549],
         [-0.2376, -0.2124],
         [-0.1112,  0.0774],
         [-0.0057,  0.2289],
         [-0.0256,  0.0764],
         [-0.0872, -0.0567]],)
input_a = torch.FloatTensor([[0.0237, 0.4910,],
        [0.1957, 0.9537, ],
        [0.6962, 0.5300, ],
        [0.3251, 0.0902, ],
        [0.5139, 0.4569, ],
        [0.0508, 0.2630, ]])
bias = 0.1441
res = kernel_a * input_a
print(torch.sum(res) + bias, '做完对位相乘') # 自己计算的结果为-0.0442，和Conv1d计算的结果a[00]对得上
